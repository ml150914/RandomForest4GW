{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b4674ae-5ce3-4e5f-900b-1e3223d3e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend in use: agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Must be the FIRST line\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "# Confirm backend\n",
    "print(\"Backend in use:\", matplotlib.get_backend())  # must print 'agg'\n",
    "\n",
    "# import modules\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# import the sklearn modules                                                                                                               \n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay, make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b17c57e3-265b-450b-b1f7-bf0101967e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "dfnoise = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/data/O3a/noise_no_catalogue.csv', sep = ',')\n",
    "dfinj = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/data/O3a/injections.csv', sep = ',')\n",
    "dfEvents_confident = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/data/O3a-events/GWTCO3a.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c622a88-0568-4bd5-8ae5-d30421f72539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots are stored in  /home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-figs/HL_F_ER_M_Best/\n",
      "Data are stored in  /home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-data/HL_F_ER_M_Best/\n"
     ]
    }
   ],
   "source": [
    "# Define the path and features\n",
    "\n",
    "# Path specifics  \n",
    "TYPE = \"DOUBLE\" # Type of coincidence\n",
    "Ifo = 'HL' # Interferometers\n",
    "F = 'F_ER' # Features\n",
    "M = 'M_Best' # Type of model\n",
    "\n",
    "\n",
    "#Define the features\n",
    "features_inj = [#'index', \n",
    "                'L_snr', 'H_snr','V_snr',#F0\n",
    "                'amplitude', #F9\n",
    "                'L_autochi^2_PQ',\t'H_autochi^2_PQ', #F0\n",
    "                'm1', 'm2', #F1\n",
    "                'mc',\n",
    "                's1z', 's2z', #F2\n",
    "                't_dur', #F3\n",
    "                'nEvents', #F4\n",
    "                'L_ERw', 'H_ERw', #F5\n",
    "                'L_phase',\t'H_phase', #F6\n",
    "                'L_effDist', 'H_effDist', #F7\n",
    "                'Lend_time', 'Hend_time',\n",
    "                'iFAR',\n",
    "                'label',\n",
    "                'gps_time',\n",
    "            ]\n",
    "\n",
    "savefig_path = f'/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-figs/{Ifo}_{F}_{M}/'\n",
    "savedata_path = f'/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-data/{Ifo}_{F}_{M}/'\n",
    "\n",
    "if(os.path.isdir(savefig_path) == False):\n",
    "    os.makedirs(savefig_path)\n",
    "if(os.path.isdir(savedata_path) == False):\n",
    "    os.makedirs(savedata_path)\n",
    "\n",
    "print('Plots are stored in ',savefig_path )\n",
    "print('Data are stored in ', savedata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f32edae1-603b-4fcc-83e2-c7c6144e3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create balanced dataset\n",
    "dfDataset_filtered1 = dfnoise.sample(n=83528, replace=False, random_state=57) # Dataset for Test - Training\n",
    "#dfDataset_filtered2 = dfDataset_filtered[~dfDataset_filtered.index.isin(dfDataset_filtered1.index)]\n",
    "dfDataset_filtered2 = dfnoise.drop(dfDataset_filtered1.index) # Dataset for Validation\n",
    "#dfDataset_filtered = dfDataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2d385d0-4c1f-47b0-a046-571b0dc092cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise    83528\n",
      "Inj      83528\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the final dataset \n",
    "dataset = pd.concat([dfDataset_filtered1, dfinj], ignore_index=True) # dataset in Training - Test \n",
    "print(dataset['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ff0fb52-760e-491c-9003-9d2f1a31e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the ranking statistics distribution\n",
    "noise_hist = dataset[dataset['label'] == 'Noise']['amplitude']\n",
    "inj_hist = dataset[dataset['label'] == 'Inj']['amplitude']\n",
    "\n",
    "# Define common bin edges (example: 50 bins over combined range)\n",
    "min_val = min(noise_hist.min(), inj_hist.min())\n",
    "max_val = max(noise_hist.max(), inj_hist.max())\n",
    "bins = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "# Create a new figure explicitly\n",
    "plt.figure()\n",
    "\n",
    "# Plot\n",
    "sns.histplot(noise_hist, bins=bins, alpha=0.7, label='Noise', stat = 'count', element = 'step')\n",
    "sns.histplot(inj_hist, bins=bins, alpha=0.5, label='Injections', stat = 'count', element = 'step')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Ranking statistic', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "plt.title('Ranking statistic Noise - Injections Distributions', fontsize=15)\n",
    "\n",
    "plt.savefig(savefig_path + 'amplitude_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()  # Close the figure to avoid accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75d0ff7e-88a4-4753-957c-68a827a27bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create addiction features (if you need)\n",
    "dataset['m_tot'] = dataset['m1'] + dataset['m2']\n",
    "dataset['s_eff'] = (dataset['m1']*dataset['s1z'] + dataset['m2']*dataset['s2z']) / dataset['m_tot']\n",
    "dataset['q'] = dataset['m2']/dataset['m1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c66491c-3ed9-45a2-ac92-8bdab400ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise    83528\n",
      "Inj      83528\n",
      "Name: label, dtype: int64\n",
      "Index(['L_snr', 'H_snr', 'V_snr', 'amplitude', 'L_autochi^2_PQ',\n",
      "       'H_autochi^2_PQ', 'm1', 'm2', 'mc', 's1z', 's2z', 't_dur', 'nEvents',\n",
      "       'L_ERw', 'H_ERw', 'L_phase', 'H_phase', 'L_effDist', 'H_effDist',\n",
      "       'Lend_time', 'Hend_time', 'iFAR', 'label', 'gps_time', 'm_tot', 's_eff',\n",
      "       'q', 'L_ER', 'H_ER', 'dphi', 'dt', 'dD'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/182385904.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset['m_tot'] = dataset['m_tot']\n",
      "/tmp/ipykernel_2157616/182385904.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset['s_eff'] = dataset['s_eff']\n",
      "/tmp/ipykernel_2157616/182385904.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset['q'] = dataset['q']\n",
      "/tmp/ipykernel_2157616/182385904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset['L_ER'] = np.sqrt(1 - dfDataset['L_ERw']) + 0.3\n",
      "/tmp/ipykernel_2157616/182385904.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset['H_ER'] = np.sqrt(1 - dfDataset['H_ERw']) + 0.3\n"
     ]
    }
   ],
   "source": [
    "# Here we work only with Training - Test dataset\n",
    "\n",
    "dfDataset = dataset[features_inj]\n",
    "dfDataset['m_tot'] = dataset['m_tot']\n",
    "dfDataset['s_eff'] = dataset['s_eff']\n",
    "dfDataset['q'] = dataset['q']\n",
    "\n",
    "dfDataset['L_ER'] = np.sqrt(1 - dfDataset['L_ERw']) + 0.3\n",
    "dfDataset['H_ER'] = np.sqrt(1 - dfDataset['H_ERw']) + 0.3\n",
    "\n",
    "dfDataset['dphi'] = dfDataset['H_phase'] - dfDataset['L_phase']\n",
    "dfDataset['dt'] = dfDataset['Hend_time'] - dfDataset['Lend_time']\n",
    "dfDataset['dD'] = dfDataset['H_effDist'] - dfDataset['L_effDist']\n",
    "#dfDataset['mc_rotated'] = dfDataset['mc'] / dfDataset['m_tot']\n",
    "#dfDataset['L_snr_rotated'] = dfDataset['L_snr'] / dfDataset['H_snr']\n",
    "\n",
    "print(dfDataset['label'].value_counts())\n",
    "print(dfDataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2649cd4-852e-4ab8-861f-999c0363cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "Index(['L_snr', 'H_snr', 'L_autochi^2_PQ', 'H_autochi^2_PQ', 'm1', 'm2', 's1z',\n",
      "       's2z', 't_dur', 'nEvents', 'L_ER', 'H_ER', 'dphi', 'dt', 'dD'],\n",
      "      dtype='object')\n",
      "Labels array:\n",
      "label\n",
      "Inj      83528\n",
      "Noise    83528\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#create the X matrix for features and y for the target  \n",
    "X = dfDataset.drop(columns = ['label','gps_time','V_snr','m_tot', 's_eff',\n",
    "                              'iFAR', 'amplitude', 'q', 'mc','L_ERw', 'H_ERw',\n",
    "                              'L_phase', 'H_phase', 'L_effDist', 'H_effDist', 'Lend_time', 'Hend_time'], axis = 1)\n",
    "\n",
    "#X.index.name = 'INDEX'\n",
    "print('Training dataset:')\n",
    "print(X.columns)\n",
    "print('Labels array:')\n",
    "y = pd.DataFrame(dfDataset['label'])\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "254ec005-8cfd-487b-85cd-a0c0c8f2c2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           L_snr      H_snr  L_autochi^2_PQ  H_autochi^2_PQ         m1  \\\n",
      "0       5.017150   5.343976        0.775441        0.824745   3.269328   \n",
      "1       5.153974   5.388456        0.884636        0.889501   2.640309   \n",
      "2       5.616759   4.892799        0.602836        0.923775   6.068903   \n",
      "3       5.723305   5.029689        0.822132        1.291943  15.561476   \n",
      "4       4.852834   5.888259        0.944329        0.838534   1.021918   \n",
      "...          ...        ...             ...             ...        ...   \n",
      "167051  6.906860   7.906440        1.541581        0.881288   4.097899   \n",
      "167052  4.857018   6.582831        0.905112        1.162780   5.036959   \n",
      "167053  5.349629  12.605415        0.598217        1.651266   3.814512   \n",
      "167054  7.573595   6.347766        1.315783        0.985199   6.361099   \n",
      "167055  7.182309   7.312851        1.040693        0.797031   4.049705   \n",
      "\n",
      "              m2       s1z       s2z       t_dur  nEvents      L_ER  H_ER  \\\n",
      "0       2.134979  0.353406  0.197054   49.810547      2.0  0.300000   0.3   \n",
      "1       1.282129  0.936255 -0.000536   72.696045      1.0  0.300000   0.3   \n",
      "2       1.453620  0.318182  0.006294   34.421875      1.0  0.575596   0.3   \n",
      "3       2.153254  0.783441 -0.775057   15.694092      2.0  0.300000   0.3   \n",
      "4       1.001836  0.020643 -0.007688  152.195068      1.0  0.300000   0.3   \n",
      "...          ...       ...       ...         ...      ...       ...   ...   \n",
      "167051  1.109415  0.560815 -0.031852   75.542236    178.0  0.300000   0.3   \n",
      "167052  2.013693  0.420563  0.312350   37.531982     15.0  0.300000   0.3   \n",
      "167053  2.078935  0.070932  0.124118   44.857910     45.0  0.300000   0.3   \n",
      "167054  1.219770  0.643987  0.026150   50.444580    148.0  0.300000   0.3   \n",
      "167055  2.004530  0.059549 -0.150094   44.122070    202.0  0.300000   0.3   \n",
      "\n",
      "            dphi        dt          dD  \n",
      "0       3.053536  0.003825 -188.634716  \n",
      "1      -3.982189  0.008265 -171.003703  \n",
      "2      -3.496072  0.000864  -93.110957  \n",
      "3       3.175443  0.006327 -167.827816  \n",
      "4      -3.227436 -0.010326  -97.067647  \n",
      "...          ...       ...         ...  \n",
      "167051 -3.546720  0.004323 -132.481423  \n",
      "167052  3.583856 -0.000999 -368.300348  \n",
      "167053  3.043027  0.008637 -498.839613  \n",
      "167054  3.663615 -0.002934  -12.434351  \n",
      "167055 -2.783594 -0.006064 -111.895023  \n",
      "\n",
      "[167056 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# split the test                                                                                                                           \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state=23\n",
    "                                                    )\n",
    "\n",
    "\n",
    "#create the dataframe to save the data\n",
    "dfX_train = pd.DataFrame(X_train)\n",
    "dfX_test = pd.DataFrame(X_test)\n",
    "dfy_train = pd.DataFrame(y_train)\n",
    "dfy_test = pd.DataFrame(y_test)\n",
    "#add the index as column \n",
    "#dfX_train['INDEX'] = dfX_train.index\n",
    "#dfX_test['INDEX'] = dfX_test.index\n",
    "#dfy_train['INDEX'] = dfy_train.index\n",
    "#dfy_test['INDEX'] = dfy_test.index\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76b4d54f-576b-4b99-9c52-31ee6be659ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# grid search\\n\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\\n\\n# Define the Random Forest model\\nclf = RandomForestClassifier(random_state=42)\\n\\ny_train_array = y_train.values.ravel()\\n\\n# define the grid\\nparam_grid = {\\n    \\'n_estimators\\': [15, 50, 100],\\n    \\'criterion\\': [\\'entropy\\', \\'gini\\'],\\n    \\'max_depth\\': [10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 5, 10],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'max_features\\': [\\'sqrt\\'],\\n}\\n\\n# Define metrics function\\nprecision = make_scorer(precision_score)\\nrecall = make_scorer(recall_score)\\nf1 = make_scorer(f1_score, pos_label=\\'Inj\\')\\n\\n# Perform the grid search using F1 score\\ngrid_search_f1 = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=f1, n_jobs=-1)\\ngrid_search_f1.fit(X_train, y_train_array)\\n\\n# Get the best parameters and scores\\nprint(\"Best parameters (F1):\", grid_search_f1.best_params_)\\nprint(\"Best F1 score:\", grid_search_f1.best_score_)\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment this for grid search\n",
    "\n",
    "\"\"\"\n",
    "# grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the Random Forest model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "y_train_array = y_train.values.ravel()\n",
    "\n",
    "# define the grid\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 50, 100],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt'],\n",
    "}\n",
    "\n",
    "# Define metrics function\n",
    "precision = make_scorer(precision_score)\n",
    "recall = make_scorer(recall_score)\n",
    "f1 = make_scorer(f1_score, pos_label='Inj')\n",
    "\n",
    "# Perform the grid search using F1 score\n",
    "grid_search_f1 = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=f1, n_jobs=-1)\n",
    "grid_search_f1.fit(X_train, y_train_array)\n",
    "\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters (F1):\", grid_search_f1.best_params_)\n",
    "print(\"Best F1 score:\", grid_search_f1.best_score_)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97edb7d2-d26d-44fe-83f4-c1f80c97c5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/876544492.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability classes:  ['Inj' 'Noise']\n",
      "              ps     pTerr\n",
      "121175  0.621688  0.378312\n",
      "59010   0.013909  0.986091\n",
      "159773  0.999953  0.000047\n",
      "28560   0.006659  0.993341\n",
      "50708   0.039521  0.960479\n",
      "...          ...       ...\n",
      "85660   1.000000  0.000000\n",
      "111046  0.935147  0.064853\n",
      "107178  1.000000  0.000000\n",
      "137192  0.904397  0.095603\n",
      "110872  1.000000  0.000000\n",
      "\n",
      "[50117 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define and fit the classifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100,\n",
    "     #bootstrap=False, # added\n",
    "     bootstrap=True,\n",
    "     random_state=52,\n",
    "     n_jobs =  -1,\n",
    "     criterion =  'entropy',\n",
    "     max_features =  'sqrt',\n",
    "     max_depth =  12,\n",
    "     #min_impurity_decrease=0.00005, # added\n",
    "     min_samples_split= 5,\n",
    "     min_samples_leaf = 1,\n",
    "     #max_leaf_nodes =  8,\n",
    "     #ccp_alpha=0.00005,\n",
    "     #class weight\n",
    "     #class_weight= {'Noise': 1, 'Inj': 2}\n",
    "     )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Probability classes: ',clf.classes_)                                                                    \n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "#define the dataframe for the probabilities\n",
    "dfprobs = pd.DataFrame(y_pred_prob, columns = ['ps', 'pTerr'])\n",
    "dfprobs.index = X_test.index\n",
    "print(dfprobs)\n",
    "\n",
    "#create the dataframe to better manage the data\n",
    "dfX_train = pd.DataFrame(X_train)\n",
    "dfy_train = pd.DataFrame(y_train)\n",
    "dfX_test = pd.DataFrame(X_test)\n",
    "dfy_test = pd.DataFrame(y_test)\n",
    "\n",
    "# Create the resulting dataset for test\n",
    "dfy_pred_prob = pd.DataFrame(y_pred_prob)\n",
    "test_dataset = pd.merge(dfX_test, dfy_test, left_index=True, right_index=True, how='inner')\n",
    "#print(len(test_dataset))\n",
    "#print(test_dataset['label'].value_counts())\n",
    "\n",
    "# Add final informations\n",
    "df_final_test = pd.merge(test_dataset, dfprobs, left_index=True, right_index=True, how='inner')\n",
    "dfRFTriggers = df_final_test\n",
    "dfRFTriggers['amplitude'] = dataset['amplitude']\n",
    "dfRFTriggers['iFAR'] = dataset['iFAR']\n",
    "dfRFTriggers['gps'] = dataset['gps_time']\n",
    "\n",
    "# Separate noise and injection \n",
    "dfRFTriggers_inj = dfRFTriggers[dfRFTriggers['label'] == 'Inj']\n",
    "dfRFTriggers_noise = dfRFTriggers[dfRFTriggers['label'] == 'Noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f7da260-5d36-46b2-bc1c-dcf87eb69485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ps distribution\n",
    "# Get both data arrays\n",
    "ps_noise = dfRFTriggers_noise['ps']\n",
    "ps_inj = dfRFTriggers_inj['ps']\n",
    "\n",
    "# Define common bins across both datasets\n",
    "min_val = min(ps_noise.min(), ps_inj.min())\n",
    "max_val = max(ps_noise.max(), ps_inj.max())\n",
    "bins = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot using consistent bins\n",
    "sns.histplot(ps_noise, bins=bins, alpha=0.5, label='Noise', stat='count', element='step')\n",
    "sns.histplot(ps_inj, bins=bins, label='Injections', stat='count', element='step')\n",
    "\n",
    "# Formatting\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p_s$', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "plt.title(r'$p_s$ Injections vs Noise Distribution', fontsize=15)\n",
    "plt.savefig(savefig_path + 'ps_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92d17b4a-b2bc-4ac5-8160-9397429f32e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/2671188877.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_inj['ps_lim'] = np.log(dfRFTriggers_inj['ps'] / (1 - dfRFTriggers_inj['ps']))\n",
      "/tmp/ipykernel_2157616/2671188877.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_noise['ps_lim'] = np.log(dfRFTriggers_noise['ps'] / (1 - dfRFTriggers_noise['ps']))\n",
      "/tmp/ipykernel_2157616/2671188877.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_inj['ps_lim'].replace([np.inf], m1, inplace=True)\n",
      "/tmp/ipykernel_2157616/2671188877.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_noise['ps_lim'].replace([np.inf], m2, inplace=True)\n",
      "/tmp/ipykernel_2157616/2671188877.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_noise['ps_lim'].replace([-np.inf], min2, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Now we apply the logit transformation - ensure no inf values\n",
    "\n",
    "# Ensure no inf values\n",
    "dfRFTriggers_inj['ps_lim'] = np.log(dfRFTriggers_inj['ps'] / (1 - dfRFTriggers_inj['ps']))\n",
    "dfRFTriggers_noise['ps_lim'] = np.log(dfRFTriggers_noise['ps'] / (1 - dfRFTriggers_noise['ps']))\n",
    "\n",
    "# Replace inf and -inf with finite max/min values\n",
    "m1 = dfRFTriggers_inj.loc[np.isfinite(dfRFTriggers_inj['ps_lim']), 'ps_lim'].max()\n",
    "dfRFTriggers_inj['ps_lim'].replace([np.inf], m1, inplace=True)\n",
    "\n",
    "m2 = dfRFTriggers_noise.loc[np.isfinite(dfRFTriggers_noise['ps_lim']), 'ps_lim'].max()\n",
    "min2 = dfRFTriggers_noise.loc[np.isfinite(dfRFTriggers_noise['ps_lim']), 'ps_lim'].min()\n",
    "dfRFTriggers_noise['ps_lim'].replace([np.inf], m2, inplace=True)\n",
    "dfRFTriggers_noise['ps_lim'].replace([-np.inf], min2, inplace=True)\n",
    "\n",
    "# Define common bins over both datasets\n",
    "combined = pd.concat([dfRFTriggers_inj['ps_lim'], dfRFTriggers_noise['ps_lim']])\n",
    "bins = np.linspace(combined.min(), combined.max(), 100)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot histograms\n",
    "sns.histplot(dfRFTriggers_noise['ps_lim'], bins=bins, label='Noise', stat='count', element='step', alpha=0.5)\n",
    "sns.histplot(dfRFTriggers_inj['ps_lim'], bins=bins, label='Injections', stat='count', element='step')\n",
    "\n",
    "# Formatting\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'$\\tilde{p}_s$')\n",
    "plt.ylabel('Occurrencies')\n",
    "plt.legend()\n",
    "plt.title(r'Distribution of $\\tilde{p}_s$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(savefig_path + 'ps_lim_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a2e8cc0-583d-4efc-ac82-650383cd0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute the KDE for the pAstro\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "values_inj = dfRFTriggers_inj['ps_lim'].values.reshape(-1,1) # create trhe right format for the data\n",
    "\n",
    "# Choose the binning\n",
    "binning_noise = 0.6\n",
    "binning_inj =0.8\n",
    "\n",
    "kde_inj = KernelDensity(bandwidth=binning_inj, kernel='gaussian') # Define the kde with banwidth and kernel\n",
    "kde_inj.fit(values_inj) # fit with the values\n",
    "\n",
    "x = np.linspace(min(dfRFTriggers_inj['ps_lim']), max(dfRFTriggers_inj['ps_lim']), 25).reshape(-1,1) # take some random values from the data\n",
    "\n",
    "# Estimate the density for the range of values\n",
    "log_density_inj = kde_inj.score_samples(x) # Comput the log likelihood oe each sample under the model\n",
    "density_inj = np.exp(log_density_inj)\n",
    "\n",
    "values_noise = dfRFTriggers_noise['ps_lim'].values.reshape(-1,1) # create trhe right format for the data\n",
    "\n",
    "kde_noise = KernelDensity(bandwidth=binning_noise, kernel='gaussian') # Define the kde with banwidth and kernel\n",
    "kde_noise.fit(values_noise) # fit with the values\n",
    "\n",
    "\n",
    "y = np.linspace(min(dfRFTriggers_noise['ps_lim']), max(dfRFTriggers_noise['ps_lim']), 25).reshape(-1,1) # take some random values from the data\n",
    "\n",
    "# Estimate the density for the range of values\n",
    "log_density_noise = kde_noise.score_samples(y) # Comput the log likelihood oe each sample under the model\n",
    "density_noise = np.exp(log_density_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33bfdf8b-032c-4ad8-948a-9580a9b50eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plt the KDE\n",
    "\n",
    "# Flatten KDE input arrays\n",
    "x = x.flatten()\n",
    "y = y.flatten()\n",
    "\n",
    "# Combine both ps_lim arrays to define shared bin edges\n",
    "ps_lim_inj = dfRFTriggers_inj['ps_lim']\n",
    "ps_lim_noise = dfRFTriggers_noise['ps_lim']\n",
    "combined = np.concatenate([ps_lim_inj, ps_lim_noise])\n",
    "bins = np.linspace(combined.min(), combined.max(), 25)\n",
    "\n",
    "# Create figure and twin axes\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot histograms with shared bins and no auto-legend\n",
    "sns.histplot(ps_lim_noise, bins=bins, alpha=0.3, ax=ax1, stat=\"density\", color='b', label=None)\n",
    "sns.histplot(ps_lim_inj, bins=bins, alpha=0.3, ax=ax1, stat=\"density\", color='r', label=None)\n",
    "\n",
    "# Plot KDEs (lines) with no auto-legend\n",
    "sns.lineplot(x=x, y=density_inj, color='r', label=None)\n",
    "sns.lineplot(x=y, y=density_noise, color='b', label=None)\n",
    "\n",
    "# Manually define legend handles\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "custom_legend = [\n",
    "    Line2D([0], [0], color='r', lw=2, label='Injections PDF'),\n",
    "    Line2D([0], [0], color='b', lw=2, label='Noise PDF'),\n",
    "    Patch(facecolor='r', alpha=0.3, label='Injections (hist)'),\n",
    "    Patch(facecolor='b', alpha=0.3, label='Noise (hist)'),\n",
    "]\n",
    "# Ticks dimension\n",
    "ax1.tick_params(axis='both', labelsize=25)  # ticks for ax1\n",
    "ax2.tick_params(axis='both', labelsize=25)  # ticks for ax2\n",
    "\n",
    "# Axis labels and title\n",
    "ax1.set_xlabel(r'$\\tilde{p}_s$',  fontsize=30)\n",
    "ax1.set_ylabel('Histogram Density',  fontsize=30)\n",
    "ax2.set_ylabel('PDF Density',  fontsize=30)\n",
    "plt.title('PDF via KDE and Histogram',  fontsize=40)\n",
    "\n",
    "# External single legend\n",
    "ax1.legend(handles=custom_legend, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=20)\n",
    "\n",
    "# Save and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(savefig_path + 'pdf_final.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29ffdaab-fc97-452d-99c8-2579a7dfd1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for Noise (RATE 6 months) 173130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/1019813341.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_inj['pAstro_rw'] = pAstro_rw_signals\n"
     ]
    }
   ],
   "source": [
    "# Now compute the pAstro - Injections\n",
    "\n",
    "values = np.array(dfRFTriggers_inj['ps_lim']) # read the ps_lim\n",
    "ranks = values.reshape(-1, 1) # reshape it\n",
    "#ranks_list = ranks.flatten().tolist() # flatten to a list\n",
    "\n",
    "log_score_event = kde_inj.score_samples(ranks) # assign the score\n",
    "score_event =  np.exp(log_score_event) # p(signal|ps_lim)\n",
    "\n",
    "log_score_noise = kde_noise.score_samples(ranks)\n",
    "score_noise = np.exp(log_score_noise) # p(noise|ps_lim)\n",
    "\n",
    "# priors\n",
    "Lambda1 = 36\n",
    "Lambda0 = len(dfnoise)\n",
    "print('Prior for Noise (RATE 6 months)', len(dfnoise))\n",
    "\n",
    "# Now compute the pAstro from ps_lim\n",
    "\n",
    "pAstro_rw_signals =[]\n",
    "for rank, score_s, score_n in zip(ranks, score_event, score_noise):\n",
    "   S = Lambda1*score_s\n",
    "   N = Lambda0*score_n\n",
    "   value = S/(S + N)\n",
    "   pAstro_rw_signals.append(value.tolist())\n",
    "\n",
    "dfRFTriggers_inj['pAstro_rw'] = pAstro_rw_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9c5b42b-d490-4c03-b049-75359969dfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/3076381244.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfRFTriggers_noise['pAstro_rw'] = pAstro_rw_noise\n"
     ]
    }
   ],
   "source": [
    "# Now compute the pAstro - Noise\n",
    "\n",
    "values = np.array(dfRFTriggers_noise['ps_lim'])\n",
    "ranks = values.reshape(-1, 1)\n",
    "\n",
    "log_score_event = kde_inj.score_samples(ranks)\n",
    "score_event =  np.exp(log_score_event)\n",
    "\n",
    "log_score_noise = kde_noise.score_samples(ranks)\n",
    "score_noise = np.exp(log_score_noise)\n",
    "\n",
    "pAstro_rw_noise =[]\n",
    "for rank, score_s, score_n in zip(ranks, score_event, score_noise):\n",
    "   S = Lambda1*score_s\n",
    "   N = Lambda0*score_n\n",
    "   value = S/(S + N)\n",
    "   pAstro_rw_noise.append(value.tolist())\n",
    "\n",
    "dfscore_event_noise_triggers = pd.DataFrame(score_event)\n",
    "dfscore_noise_noise_triggers = pd.DataFrame(score_noise)\n",
    "\n",
    "dfRFTriggers_noise['pAstro_rw'] = pAstro_rw_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f8ef42a-e9a9-4f49-be27-ce10bbf002bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dfRFTriggers_inj['amplitude'], dfRFTriggers_inj['pAstro_rw'], c = 'darkorange', s = 1, label = 'Injections')\n",
    "plt.scatter(dfRFTriggers_noise['amplitude'], dfRFTriggers_noise['pAstro_rw'], c = 'b', s = 3, label = 'Noise')\n",
    "plt.xlabel('Ranking statistic')\n",
    "plt.ylabel(r'$p_\\mathrm{astro}^{ps}$', fontsize = 15)\n",
    "plt.title(r'$p_\\mathrm{astro}^{ps}$ - ranking statistics', fontsize = 15)\n",
    "plt.legend()\n",
    "plt.savefig(savefig_path + 'pAstro_ps_amplitude_injections.png', dpi=300, bbox_inches='tight')\n",
    "plt.legend()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d157c91e-2d9c-4824-86ec-5ed5a5937c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the the RoC function \n",
    "def RoC(df1, df2, column, var, label, save = True, thr=0.005, color = 'b'):\n",
    "    if column not in df1.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
    "# Drop missing values and sort unique values of the column                                                                                                       \n",
    "    data1 = df1[column].dropna()\n",
    "    data2 = df2[column].dropna()\n",
    "    #thresholds = np.sort(data.unique())                                                                                                                             \n",
    "    thresholds = np.arange(min(data1),max(data1), thr)\n",
    "\n",
    "    # Calculate the cumulative distribution                                                                                                                          \n",
    "    count_above_threshold1 = [np.sum(data1 > threshold) / len(data1) for threshold in thresholds]\n",
    "    count_above_threshold2 =  [np.sum(data2 > threshold) / len(data2) for threshold in thresholds]\n",
    "\n",
    "    # Create a DataFrame for the cumulative distribution                                                                                                             \n",
    "    cumulative_df = pd.DataFrame({\n",
    "        'threshold': thresholds,\n",
    "        'count_above_threshold1': count_above_threshold1,\n",
    "        'count_above_threshold2': count_above_threshold2\n",
    "    })\n",
    "\n",
    "    if save:\n",
    "        cumulative_df.to_csv(path_data + f'/{var}_RoC_{label}.csv', index=False)\n",
    "\n",
    "    # Plot the cumulative distribution                                                                                                                               \n",
    "    plt.plot(cumulative_df['count_above_threshold1'], cumulative_df['count_above_threshold2'], label=label, c = color, linewidth=3.0)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "\n",
    "    return cumulative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "741973f7-2d6f-4e86-9352-6dfa140f3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-figs/HL_F_ER_M_Best/\n"
     ]
    }
   ],
   "source": [
    "# Compute the Roc function  \n",
    "plt.figure()\n",
    "RoC(dfRFTriggers_noise, dfRFTriggers_inj, 'ps', 'Roc', r'$p_s$', thr = 0.0001, color = 'r', save = False)\n",
    "RoC(dfRFTriggers_noise,dfRFTriggers_inj, 'amplitude', 'Roc', 'Ranking statistic', thr = 0.005, save = False, color= 'steelblue')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.ylabel('Nd (density)', fontsize=15)\n",
    "plt.xlabel(r'$\\alpha$', fontsize=15)\n",
    "plt.savefig(savefig_path + 'RoC_O3a.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(savefig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60c57e6d-6333-49f8-83ba-302366cb6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now study the pAstro for events in the catalogue\n",
    "\n",
    "events = dfEvents_confident#dfDataset_events_complete\n",
    "events['label'] = 'event'\n",
    "\n",
    "# fill in the added features (if you want to use them)\n",
    "events['L_ER'] = np.sqrt(1 - events['L_ERw']) + 0.3\n",
    "events['H_ER'] = np.sqrt(1 - events['H_ERw']) + 0.3\n",
    "events['dphi'] = events['H_phase'] - events['L_phase']\n",
    "events['dt'] = events['Hend_time'] - events['Lend_time']\n",
    "events['dD'] = events['H_effDist'] - events['L_effDist']\n",
    "events['m_tot'] = events['m1'] + events['m2']\n",
    "events['s_eff'] = (events['m1']*events['s1z'] + events['m2']*events['s2z']) / events['m_tot']\n",
    "\n",
    "y_evt = events['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47ac90c3-7ad8-4abd-b540-de9ad2d35d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_event_test = events.drop(columns = ['Unnamed: 0','label','gps_time', 'mc', 'm_tot', 's_eff','V_snr',\n",
    "                               'iFAR', 'amplitude',\n",
    "                              'L_phase', 'H_phase', 'L_effDist', 'H_effDist', 'Lend_time', 'Hend_time','L_ERw', 'H_ERw',\n",
    "                               'gps_time', 'commonName', 'GPS', 'pAstroMbta', 'GPS_int', 'gps_time_int'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ecc9826-b11d-49f4-9f5d-f1f7fb00caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the classifier\n",
    "y_pred_prob_events = clf.predict_proba(X_event_test)\n",
    "dfy_pred_prob_events = pd.DataFrame(y_pred_prob_events, columns = ['ps', 'pTerr'])\n",
    "\n",
    "# Compactify the result\n",
    "dfX_test_events = pd.DataFrame(X_event_test)\n",
    "dfy_test_events = pd.DataFrame(y_evt)\n",
    "\n",
    "test_dataset_events = pd.merge(dfX_test_events, dfy_test_events, left_index=True, right_index=True, how='inner')\n",
    "dfRFTriggers_events= pd.merge(test_dataset_events, dfy_pred_prob_events, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# fill in the dataframe\n",
    "dfRFTriggers_events.index = dfEvents_confident.index #dfDataset_events_complete.index\n",
    "dfRFTriggers_events['p_astro'] = dfEvents_confident['pAstroMbta']\n",
    "dfRFTriggers_events['amplitude'] = dfEvents_confident['amplitude']\n",
    "dfRFTriggers_events['iFAR'] = dfEvents_confident['iFAR']\n",
    "dfRFTriggers_events['commonName'] = dfEvents_confident['commonName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46c81c71-69fb-43e7-befa-528aeb5d9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "    pAstro_rw     ps_lim\n",
      "0    1.000000  10.762128\n",
      "1    1.000000  10.762128\n",
      "2    1.000000  10.762128\n",
      "3    1.000000  10.762128\n",
      "4    1.000000   8.627789\n",
      "5    1.000000  10.762128\n",
      "6    1.000000  10.762128\n",
      "7    1.000000  10.762128\n",
      "8    1.000000  10.762128\n",
      "9    1.000000  10.762128\n",
      "10   0.040520   3.759374\n",
      "11   1.000000  10.762128\n",
      "12   1.000000   9.995415\n",
      "13   0.786194   6.472326\n",
      "14   1.000000  10.762128\n",
      "15   1.000000  10.132898\n",
      "16   1.000000  10.762128\n",
      "17   1.000000  10.762128\n",
      "18   1.000000  10.762128\n",
      "19   0.084162   4.852095\n",
      "20   1.000000  10.762128\n",
      "21   0.098128   5.074166\n",
      "22   0.041002   3.769493\n"
     ]
    }
   ],
   "source": [
    "# Now compute the ps_lim for the events\n",
    "dfRFTriggers_events['ps_lim'] = np.log(dfRFTriggers_events['ps'] / (1 - dfRFTriggers_events['ps']))\n",
    "\n",
    "m1 = dfRFTriggers_events.loc[dfRFTriggers_events['ps_lim'] != np.inf, 'ps_lim'].max()\n",
    "dfRFTriggers_events['ps_lim'] = dfRFTriggers_events['ps_lim'].replace([np.inf],m1)\n",
    "\n",
    "# Compute pAstro for real Triggers\n",
    "\n",
    "values = np.array(dfRFTriggers_events['ps_lim'])\n",
    "ranks = values.reshape(-1, 1)\n",
    "ranks_list = ranks.flatten().tolist()\n",
    "\n",
    "log_score_event = kde_inj.score_samples(ranks)\n",
    "score_event =  np.exp(log_score_event)\n",
    "\n",
    "\n",
    "log_score_noise = kde_noise.score_samples(ranks)\n",
    "score_noise = np.exp(log_score_noise)\n",
    "\n",
    "\n",
    "Lambda1 = 36\n",
    "#Lambda0 = len(noise)\n",
    "print(len(score_event))\n",
    "print(len(score_noise))\n",
    "print(len(ranks_list))\n",
    "\n",
    "\n",
    "pAstro_rw_signals =[]\n",
    "for rank, score_s, score_n in zip(ranks, score_event, score_noise):\n",
    "   S = Lambda1*score_s\n",
    "   N = Lambda0*score_n\n",
    "   value = S/(S + N)\n",
    "   pAstro_rw_signals.append(value.tolist())\n",
    "\n",
    "\n",
    "dfRFTriggers_events['pAstro_rw'] = pAstro_rw_signals\n",
    "print(dfRFTriggers_events[['pAstro_rw', 'ps_lim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac747d3c-9c02-48a7-a1f0-841c52bae843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final result\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dfRFTriggers_events['amplitude'], dfRFTriggers_events['pAstro_rw'], label = r'Events - $p_\\mathrm{astro}^{ps}$', s = 55)\n",
    "plt.scatter(dfRFTriggers_events['amplitude'], dfRFTriggers_events['p_astro'], label = r'Events - $p_\\mathrm{astro}$', color = 'r', marker='x', s = 55)\n",
    "plt.scatter(dfRFTriggers_events[dfRFTriggers_events['commonName'] == 'GW190924_021846']['amplitude'], dfRFTriggers_events[dfRFTriggers_events['commonName'] == 'GW190924_021846']['pAstro_rw'], label = 'GW190924_021846', color = 'g', s = 55)\n",
    "plt.legend()\n",
    "plt.title(r'$p_\\mathrm{astro}^{ps}$ - $p_\\mathrm{astro}$ - ranking statistics O3a Events')\n",
    "plt.xlabel('ranking statistics')\n",
    "plt.ylabel(r'$p_\\mathrm{astro}$')\n",
    "plt.ylim([-0.1,1.1])\n",
    "# Plot a horizontal line at y = 25\n",
    "plt.axhline(y=0.5, color='g', linestyle='-.')\n",
    "plt.savefig(savefig_path + 'pAstro_ps_amplitude_O3aEvents_hihglight.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d280406c-9dda-44cb-a58a-b6826fa35193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------O3b consistency test\n",
    "\"\"\"\n",
    "path_data = '/home/lorenzo.mobilia/PhD-Thesis/RF-codes/data/O3b/'\n",
    "\n",
    "\n",
    "NoiseHL_BBH = 'Triggers_noise_values_DT001_RealNoise_MbtaHL_Von_BBH.txt'\n",
    "NoiseHL_BNS = 'Triggers_noise_values_DT001_RealNoise_MbtaHL_Von_BNS.txt'\n",
    "NoiseHL_NSBH = 'Triggers_noise_values_DT001_RealNoise_MbtaHL_Von_NSBH.txt'\n",
    "InjHL_BBH = 'Triggers_injection_values_DT001_RealNoise_BBH_MbtaHL_Von.txt'\n",
    "InjHL_BNS = 'Triggers_injection_values_DT001_RealNoise_BNS_MbtaHL_Von.txt'\n",
    "InjHL_NSBH = 'Triggers_injection_values_DT001_RealNoise_NSBH_MbtaHL_Von.txt'\n",
    "\n",
    "\n",
    "noise_BBH = pd.read_csv(path_data + NoiseHL_BBH, delimiter='\\t', index_col = False)\n",
    "noise_BNS = pd.read_csv(path_data + NoiseHL_BNS, delimiter='\\t', index_col = False)\n",
    "noise_NSBH = pd.read_csv(path_data + NoiseHL_NSBH, delimiter='\\t', index_col = False)\n",
    "\n",
    "#.sample(n=83528)\n",
    "noise = pd.concat([noise_BBH, noise_BNS, noise_NSBH], ignore_index=True)\n",
    "\n",
    "injHL_BBH = pd.read_csv(path_data + InjHL_BBH, delimiter = '\\t', index_col = False)\n",
    "injHL_BNS = pd.read_csv(path_data + InjHL_BNS, delimiter = '\\t', index_col = False)\n",
    "injHL_NSBH = pd.read_csv(path_data + InjHL_NSBH, delimiter = '\\t', index_col = False)\n",
    "\n",
    "#inj = inj[inj['amplitude']<9]\n",
    "noise['label'] = 'Noise'\n",
    "injHL_BBH['label'] = 'Inj'\n",
    "injHL_BNS['label'] = 'Inj'\n",
    "injHL_NSBH['label'] = 'Inj'\n",
    "\"\"\"\n",
    "# upload the datasets  \n",
    "Ifo = 'HL'\n",
    "F = 'F0_ER_CONSISTENCY_TEST_O3b'\n",
    "M = 'M_best' # result in paper are saved in M. - Good results, you fin in M_best - F0 the results with best model + ER. M_best + F0_noER best model + noER\n",
    "\n",
    "#Define the features\n",
    "features_inj = [#'index', \n",
    "                'L_snr', 'H_snr','V_snr',#F0\n",
    "                'amplitude', #F9\n",
    "                'L_autochi^2_PQ',\t'H_autochi^2_PQ', #F0\n",
    "                'm1', 'm2', #F1\n",
    "                'mc',\n",
    "                's1z', 's2z', #F2\n",
    "                't_dur', #F3\n",
    "                'nEvents', #F4\n",
    "                'L_ERw', 'H_ERw', #F5\n",
    "                'L_phase',\t'H_phase', #F6\n",
    "                'L_effDist', 'H_effDist', #F7\n",
    "                'Lend_time', 'Hend_time',\n",
    "                'iFAR',\n",
    "                'label',\n",
    "                'gps_time',\n",
    "            ]\n",
    "\n",
    "savefig_path = f'/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-figs/{Ifo}_{F}_{M}/'\n",
    "savedata_path = f'/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/O3a-results-data/{Ifo}_{F}_{M}/'\n",
    "\n",
    "if(os.path.isdir(savefig_path) == False):\n",
    "    os.makedirs(savefig_path)\n",
    "if(os.path.isdir(savedata_path) == False):\n",
    "    os.makedirs(savedata_path)\n",
    "\n",
    "print('Plots are stored in ',savefig_path )\n",
    "print('Data are stored in ', savedata_path)\n",
    "\n",
    "\n",
    "dfnoise = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/data/O3b/noise_no_catalogue.csv', sep = ',')\n",
    "dfinj = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/pastro-o3-mbta-triggers-paper/RF-codes/data/O3b/injections.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749607-dcc0-4a46-a0ff-d1f9b6b49cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, clear the dataset\n",
    "\n",
    "# Create the dataset for the Events\n",
    "\"\"\"\n",
    "noise = noise[features_inj]\n",
    "\n",
    "dfEvents = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/RF-codes/data/O3b-events/GWTC-3-confident.csv', delimiter = ';', usecols=['GPS', 'pAstroMbta', 'commonName'])\n",
    "print('Events before cut in pAstro', len(dfEvents))\n",
    "dfEvents = dfEvents.dropna(subset=['pAstroMbta'])\n",
    "# Remove the events with pAstroMBTA == 0\n",
    "\n",
    "dfEvents = dfEvents[dfEvents['pAstroMbta'] != 0]\n",
    "\n",
    "dfEvents['GPS_int'] = dfEvents['GPS'].astype(int)\n",
    "noise['gps_time_int'] = noise['gps_time'].astype(int)\n",
    "\n",
    "print(dfEvents[dfEvents['commonName'] == 'GW200208_222617']['GPS_int'])\n",
    "dfEvents.at[20, 'GPS_int'] = 1265235996\n",
    "noise = noise.drop(100520)\n",
    "print(noise[noise['gps_time_int'] == 1265235996])\n",
    "\n",
    "# Remove those events from the dataset\n",
    "dfDataset_filtered = noise[~noise['gps_time_int'].isin(dfEvents['GPS_int'])]\n",
    "\n",
    "dfDataset_events = noise[noise['gps_time_int'].isin(dfEvents['GPS_int'])]\n",
    "#row = noise[noise.index == 26937]\n",
    "dfDataset_events_complete = dfDataset_events.merge(dfEvents, left_on='gps_time_int', right_on='GPS_int', how = 'inner')\n",
    "#dfDataset_events_complete = pd.concat([dfDataset_events_complete,row], ignore_index = True) #Manually add the event GW190727_060333\n",
    "dfDataset_events_pastro = dfDataset_events_complete['pAstroMbta']\n",
    "dfDataset_events_complete = dfDataset_events_complete.drop_duplicates(subset=['commonName'])\n",
    "#dfDataset_events_complete = dfDataset_events_complete.fillna('GW190727_060333')\n",
    "\n",
    "#print(dfDataset_events_complete.head())\n",
    "#print(dfDataset_events_complete.head())\n",
    "\n",
    "# Now remove ALL the events present in the catalogue (GWTC2.1 Confident)\n",
    "\n",
    "dfEvents_confident = pd.read_csv('/home/lorenzo.mobilia/PhD-Thesis/RF-codes/data/O3b-events/GWTC-3-confident.csv', delimiter = ';')\n",
    "print('Events before cut in pAstro', len(dfEvents_confident))\n",
    "\n",
    "dfEvents_confident['GPS_int'] = dfEvents_confident['GPS'].astype(int)\n",
    "# Remove those events from the dataset\n",
    "dfDataset_filtered = dfDataset_filtered[~dfDataset_filtered['gps_time_int'].isin(dfEvents_confident['GPS_int'])]\n",
    "print('Final Dataset', dfDataset_filtered['label'].value_counts())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a51575fa-b496-461d-b7e0-416ab2afbdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise    76389\n",
      "Inj      76389\n",
      "Name: label, dtype: int64\n",
      "Noise    76389\n",
      "Inj      76389\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2157616/1834712989.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['m_tot'] = dataset_O3b['m_tot']\n",
      "/tmp/ipykernel_2157616/1834712989.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['s_eff'] = dataset_O3b['s_eff']\n",
      "/tmp/ipykernel_2157616/1834712989.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['q'] = dataset_O3b['q']\n",
      "/tmp/ipykernel_2157616/1834712989.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['L_ER'] = np.sqrt(1 - dfDataset_O3b['L_ERw']) + 0.3\n",
      "/tmp/ipykernel_2157616/1834712989.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['H_ER'] = np.sqrt(1 - dfDataset_O3b['H_ERw']) + 0.3\n",
      "/tmp/ipykernel_2157616/1834712989.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['dphi'] = dfDataset_O3b['H_phase'] - dfDataset_O3b['L_phase']\n",
      "/tmp/ipykernel_2157616/1834712989.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['dt'] = dfDataset_O3b['Hend_time'] - dfDataset_O3b['Lend_time']\n",
      "/tmp/ipykernel_2157616/1834712989.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDataset_O3b['dD'] = dfDataset_O3b['H_effDist'] - dfDataset_O3b['L_effDist']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the injections\n",
    "\"\"\"\n",
    "inj = pd.concat([injHL_BBH, injHL_BNS, injHL_NSBH])\n",
    "# Drop the '||' column along with the Inj* columns\n",
    "cols_to_drop = ['||', 'InjIndex', 'InjM1', 'InjM2', 'InjS1', 'InjS2', 'InjMc', 'InjDist', 'InjGps', 'Coinc', 't_index', 'rank_dphi', 'rank_effDistRatio', 'rank_dt', 'L_rank', 'H_rank']\n",
    "inj = inj.drop(columns=cols_to_drop)\n",
    "noise = dfDataset_filtered[features_inj]\n",
    "\"\"\"\n",
    "\n",
    "noise_balanced = dfnoise.sample(n=76389, replace=False)\n",
    "\n",
    "# Create a balanced dataset \n",
    "dataset_O3b = pd.concat([noise_balanced, dfinj], ignore_index=True)\n",
    "print(dataset_O3b['label'].value_counts())\n",
    "\n",
    "dataset_O3b['m_tot'] = dataset_O3b['m1'] + dataset_O3b['m2']\n",
    "dataset_O3b['s_eff'] = (dataset_O3b['m1']*dataset_O3b['s1z'] + dataset_O3b['m2']*dataset_O3b['s2z']) / dataset_O3b['m_tot']\n",
    "dataset_O3b['q'] = dataset_O3b['m2']/dataset_O3b['m1']\n",
    "\n",
    "dfDataset_O3b = dataset_O3b[features_inj]\n",
    "dfDataset_O3b['m_tot'] = dataset_O3b['m_tot']\n",
    "dfDataset_O3b['s_eff'] = dataset_O3b['s_eff']\n",
    "dfDataset_O3b['q'] = dataset_O3b['q']\n",
    "\n",
    "dfDataset_O3b['L_ER'] = np.sqrt(1 - dfDataset_O3b['L_ERw']) + 0.3\n",
    "dfDataset_O3b['H_ER'] = np.sqrt(1 - dfDataset_O3b['H_ERw']) + 0.3\n",
    "\n",
    "dfDataset_O3b['dphi'] = dfDataset_O3b['H_phase'] - dfDataset_O3b['L_phase']\n",
    "dfDataset_O3b['dt'] = dfDataset_O3b['Hend_time'] - dfDataset_O3b['Lend_time']\n",
    "dfDataset_O3b['dD'] = dfDataset_O3b['H_effDist'] - dfDataset_O3b['L_effDist']\n",
    "\n",
    "print(dfDataset_O3b['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec707e98-8b78-48b3-b948-e6705af0ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_O3b = dfDataset.drop(columns = ['label','gps_time', 'm_tot', 'mc', 'q', 's_eff','V_snr',\n",
    "                              'L_ERw', 'H_ERw', 'iFAR','amplitude',\n",
    "                              'L_phase', 'H_phase', 'L_effDist', 'H_effDist', 'Lend_time', 'Hend_time'], axis = 1)\n",
    "\n",
    "dfX_O3b = pd.DataFrame(X_O3b)\n",
    "#X.index.name = 'INDEX'\n",
    "print('Training dataset:')\n",
    "print(X_O3b.columns)\n",
    "print('Labels array:')\n",
    "dfy_O3b = pd.DataFrame(dfDataset_O3b['label'])\n",
    "#print(y.value_counts())\n",
    "\n",
    "y_pred_prob_O3b = clf.predict_proba(X_O3b)\n",
    "dfprobs_O3b = pd.DataFrame(y_pred_prob_O3b, columns = ['ps', 'pTerr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1ab6b-ae83-4997-ab80-787174502863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy_pred_prob_O3b = pd.DataFrame(y_pred_prob_O3b)\n",
    "test_dataset_O3b = pd.merge(dfX_O3b, dfy_O3b, left_index=True, right_index=True, how='inner')\n",
    "df_final_test_O3b = pd.merge(test_dataset_O3b, dfprobs_O3b, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "dfRFTriggers_O3b = df_final_test_O3b\n",
    "dfRFTriggers_O3b['amplitude'] = dataset_O3b['amplitude']\n",
    "dfRFTriggers_O3b['iFAR'] = dataset_O3b['iFAR']\n",
    "dfRFTriggers_O3b['gps'] = dataset_O3b['gps_time']\n",
    "\n",
    "dfRFTriggers_inj_O3b = dfRFTriggers_O3b[dfRFTriggers_O3b['label'] == 'Inj']\n",
    "dfRFTriggers_noise_O3b = dfRFTriggers_O3b[dfRFTriggers_O3b['label'] == 'Noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e80e2-cd6a-449f-9619-033db2813760",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "RoC(dfRFTriggers_noise_O3b, dfRFTriggers_inj_O3b, 'ps', 'Roc', r'$p_s$', thr = 0.0001, color = 'r', save = False)\n",
    "RoC(dfRFTriggers_noise_O3b, dfRFTriggers_inj_O3b, 'amplitude', 'Roc', 'Ranking statistic', thr = 0.005, save = False, color= 'steelblue')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.ylabel('Nd (density)', fontsize=15)\n",
    "plt.xlabel(r'$\\alpha$', fontsize=15)\n",
    "plt.savefig(savefig_path + 'RoC_O3b_O3aModel.png', dpi= 300,  bbox_inches = 'tight')\n",
    "print(savefig_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e8556-4292-4667-a637-0365d35e1b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igwn-py38",
   "language": "python",
   "name": "igwn-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
